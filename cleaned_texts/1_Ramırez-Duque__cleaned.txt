Abstract
Autism spectrum disorder (ASD) is a neurodevelopmental disorder that affects people from birth, whose symptoms are
found in the early developmental period. The ASD diagnosis is usually performed through several sessions of behavioral
observation, exhaustive screening, and manual coding behavior. The early detection of ASD signs in naturalistic behavioral
observation may be improved through Child-Robot Interaction (CRI) and technological-based tools for automated behavior
assessment. Robot-assisted tools using CRI theories have been of interest in intervention for children with Autism Spectrum
Disorder (CwASD), elucidating faster and more significant gains from the diagnosis and therapeutic intervention when
compared to classical methods. Additionally, using computer vision to analyze child’s behaviors and automated video coding
to summarize the responses would help clinicians to reduce the delay of ASD diagnosis. In this article, a CRI to enhance
the traditional tools for ASD diagnosis is proposed. The system relies on computer vision and an unstructured and scalable
network of RGBD sensors built upon Robot Operating System (ROS) and machine learning algorithms for automated face
analysis. Also, a proof of concept is presented, with participation of three typically developing (TD) children and three
children in risk of suffering from ASD.
Keywords Child-Robot interaction · Autism spectrum disorder · Convolutional neural network · Robot reasoning model ·
Statistical shape modeling
1 Introduction
Research in Child-Robot Interaction (CRI) aims to provide
the necessary conditions for the interaction between a
child and a robotic device taking into account some
fundamental features, such as child’s neurophysical and
physical condition, and the child’s mental health [1].
That is how Robot-Assisted Therapies (RAT) using CRI
theories have been of interest as an intervention for
CwASD, elucidating faster and more significant gains from
the therapeutic intervention when compared to traditional
therapies [2–4].
ASD is a neurodevelopmental disorder that affects people
from birth, and its symptoms are found in the early
� Andr´es A. Ram´ırez-Duque

Universidade Federal do Espir´ıto Santo., Av. Fernando Ferrari,
514 (29075-910), Vitoria, Brazil
developmental period. Individuals suffering from ASD
exhibit persistent deficits in social communication, social
interaction and repetitive patterns of behavior, interests, or
activities [5]. Some of the ASD signs may be observed
before the age of 10 months, although a reliable diagnosis
can only be performed at 18 months of age, according to [6],
or 24 months according to [7].
The use of computer vision to analyze the child’s
behaviors, and automated video coding to summarize the
interventions, can help the clinicians to reduce the delay
of ASD diagnosis, providing the CwASD with access
to early therapeutic interventions. In addition, CRI-based
intervention can transform traditional diagnosis methods
through a robotic device to systematically elicit child’s
behaviors that exhibit ASD signs [8].
Some of the first systems developed to assist ASD
therapists and make diagnosis based on robotic devices
have primarily been open loop and remotely operated sys-
tems. However, these approaches are unable to perform
autonomous feedback to enhance the interaction [9–11].
Journal of Intelligent & Robotic Systems (2019) 96:267–281
/ Published online: 29
March
Content courtesy of Springer Nature, terms of use apply. Rights reserved.

Nevertheless, different systems are able to modify the
behavior of the robot according to environmental interac-
tions and the child’s response, using a closed-loop and
artificial cognition approaches [12–16]. These systems have
been hypothesized to offer technological mechanisms for
supporting more flexible and potentially more naturalistic
interaction [17]. In fact, literature reports that automatic
robot’s social behaviors modulation according to specifics
scenarios has a strong effect on child’s social behavior
[12]. However, despite the increase of positive evidence,
this technology has rarely been applied to specific ASD
diagnosis.
This work aims to present a robot-assisted framework
using an artificial reasoning module to assist clinicians with
the ASD diagnostic process. The framework is composed of
a responsive robotic platform, a flexible and scalable vision
sensor network, and an automated face analysis algorithm
based on machine learning models. In this research we take
advantage of some neural models available as open sources
projects to build a completely new pipeline algorithm for
global recognition and tracking of child’s face among many
faces present in a typical unstructured clinical intervention,
in order to estimate the child’s visual focus of attention
along the time. The proposed system can be used in different
behavioral analysis scenarios typical of an ASD diagnostic
process. In order to illustrate the feasibility of the proposed
system, in this paper an experimental trial to assess joint-
attention behavior is presented employing an in-clinic setup
(unstructured environment).
The main contributions of this paper are: (i) the
development of a new artificial reasoning module upon
a flexible and scalable ROS-based vision system using
state-of-the-art machine learning neural models; (ii) the
proposal and implementation of a supervised CRI (child-
robot interaction) based on an open source social robotic
platform to enhance the traditional tools for ASD diagnosis
using an in-clinic setup protocol. For the best of our
knowledge, there are no open source projects available for
face analysis based on a multi-camera approach using ROS
with the characteristics described in our research.
2 Related Work
Recent researches have shown the acceptance and efficiency
of
technologies used
as
auxiliary tools for
therapy
and teaching of individuals with ASD [18–21]. Such
technologies may also be useful for people surrounding
ASD individuals (therapists, caregivers, family members).
For example, the use of artificial vision systems to measure
and analyze the child’s behavior can lead to alternative
screening and monitoring tools that help the clinicians to
get feedback from the effectiveness of the intervention [22].
Additionally, social robots have great potential for aid in
the diagnosis and therapy of children with ASD [18, 23].
A higher degree of control, prediction and simplicity may
be achieved in interactions with robots, impacting directly
on frustration and reducing the anxiety of these individuals
Respect to the use of computer vision techniques,
previous studies already analyzed child’s behaviors, such
as visual attention, eye gaze, eye contact, smile events, and
visual exploration using cameras and eye trackers [25, 26]
and RGBd cameras [27, 28]. These studies have shown
the potential of vision systems in improving the behavioral
coding in ASD therapies. However, these studies did not
implement techniques of CRI to enhance the intervention.
On the other hand, studies about how CwASD respond
to a robot mediator compared to human mediator have
been reported, such as intervention scenarios with imitation
games [29, 30], telling stories [9] and free play tasks
[12, 31]. These works used features, such as proxemics,
body gestures, visual contact and eye gaze as behavioral
descriptors, whereas the behavior analysis was estimated
using manual video coding.
Researchers of Vanderbilt University published a series
of research showing an experimental protocol to assess joint
attention (JA) tasks defined as the capacity for coordinated
orientation of two people toward an object or event [6].
The protocol consisted of directing the attention of the
child towards objects located in the room through adaptive
prompts [32]. Bekele et al. inferred the participant’s eye
gaze by the head pose, which was calculated in real-time by
an IR camera array [17]. In their last works, Zheng et al. and
Warren et al. used a commercial eye tracker to estimated the
children’s eye gaze around the robot and manual behavioral
coding for global evaluation [10, 33]. However, eye tracker
devices require pre-calibration and may limit the movement
of the individual. The results of these works showed that the
robot attracted children’s attention and that CwASD reached
all JA task. Nevertheless, developing JA tasks is more
difficult with a robot than with humans [10]. Anzalone et al.
developed a CRI scenario using the NAO robot to perform
JA tasks, in which the authors used an RGBD camera
to estimate only body and head movements. The results
showed that JA performance of children with ASD was
similar to the performance of TD children when interacting
with the human mediator, however, with a robot mediator,
the children with ASD presented a lower performance than
the TD children, i.e, the children with ASD needed more
social cues to finalize the task [34]. Chevalier et al. analyzed
in their study, some features, such as proprioceptive and
visual integration in CwASD, using an RGBD sensor
to record the interventions sessions and manual behavior
coding to analyzed the participants’ performance [35]. In
none of the previous works, a closed-loop subsystem was
J Intell Robot Syst (2019) 96:267–281
Content courtesy of Springer Nature, terms of use apply. Rights reserved.

implemented to provide some level of artificial cognition to
enable automated robot behavior.
In contrast with the aforementioned researches, other
works implemented automated face analysis and artificial
cognition through robot-mediator and computer vision,
which analyzed child’s engagement [36, 37], emotions
recognition capability [13, 15, 38] and child’s intentions
[14, 16]. In these works, two different strategies were
implemented, where the most common is based on mono-
camera approach using an external RGB or RGBd sensor
[15, 36, 37] or using on-board RGB cameras mounted
on the robotic-platform [13, 16]. Other strategies are
based on a highly structured environment composed of
an external camera plus an on-board camera [38] or a
network of vision sensors attached to a small table [14].
These strategies based on multi-camera methods improve
the system’s performance, but remain constrained in relation
to desired features, such as flexibility, scalability, and
modularity. Thus, despite the potential that these techniques
have shown, achieving automated child’s behavior analysis
in a naturalistic way into unstructured clinical-setups with
robots that interact accordingly remains a challenge in CRI.
3 System Architecture Overview
The ROS system used in this work is a flexible and
scalable open framework for writing modular robot-
centered systems. Similar to a computing operating system,
ROS manages the interface between robot hardware and
software modules and provides common device drivers, data
structures and tool-based packages, such as visualization
and debugging tools. In addition, ROS uses an interface
definition language (IDL) to describe the messages sent
between process or nodes, this feature facilitates the multi-
language (C++, Python and Lisp) development [39].
The overall system developed here was built using a
node graph architecture, taking advantages of the principal
ROS design criteria. As with ROS, our system consists of
a number of nodes to local video processing together a
robot’s behavior estimation, distributed around a number
of different hosts and connected at runtime in a peer-to-
peer topology. The inter-node connection is implemented
as a hand-shaking and occurs in XML-RPC protocol
along with a web-socket communication for robot’s web-
based node (/ONO node, see 
is flexible, scalable and can be dynamically modified,
i.e., each node can be started and left running along an
experimental session or resumed and connected to each
other at runtime. In addition, from a general perspective,
any robotic platform with web-socket communication can
be integrated. The developed system is composed of two
interconnected modules as shown in 
reasoning module and a CRI-channel module. The module
architectures are detailed in the following subsections.
3.1 Architecture of Reasoning Module
In this module, a distributed architecture for local video
processing is implemented. The data of each RGBD sensor
in the multi-camera system are processed for two nodes,
in which the first is a driver level node and the second
is a processing node. The driver1 node transforms the
streaming data of the RGBD sensor into the ROS messages
format. The driver addresses the data through a specialized
transport provided by plugings to publishes images in a
compressed representations while the receptor node only
sees sensor msgs/Image messages. The data processing
node executes the face analysis algorithm. This node uses
a image transport subscriber and a ROS packages called
CvBridge to turn the data into a image format supported for
the typical computer vision algorithms. Later, the same node
publishes the head pose and eye gaze direction by means of
a ROS navigation message defined as nav msgs/Odometry.
An additional node hosted in the most powerful
workstation carries out a data fusion of all navigation
messages that were generated in the local processing
stage. In addition to the fusion, this node computes the
visual focus of attention (VFOA) and publishes this as a
std msgs/Header, in which the time stamp and the target
name of the VFOA estimation are registered.
3.2 Architecture of CRI-Channel
The system proposed here has two bidirectional communi-
cation channels, a robot-device, and a web-based applica-
tion to interact with both the child and the therapist. The
robot device can interact with the CwASD executing differ-
ent physical actions, such as facial expression, upper limb
poses, and verbal communication. Thus, according to the
child’s performance, the reasoning module can modify the
robot’s behavior through automatic gaze shifting, chang-
ing the facial expression and providing sound rewards. The
client-side application was developed to allow the therapist
to control and register all step of the intervention proto-
col. This interface was also used to supervise and control
the robot’s behavior and to offer feedback to the therapist
about the child’s performance along the intervention. This
App has two channels of communication for interacting
with the reasoning module. The first connection uses a web-
socket protocol and a RosBridge suite package to support
the interpretation of ROS messages, as well as, JSON-based
commands in ROS. The second one uses a ROS module
1Tools for using the Kinect One (Kinect V2) in ROS, 
com/code-iai/iai kinect2.
J Intell Robot Syst (2019) 96:267–281
Content courtesy of Springer Nature, terms of use apply. Rights reserved.

of the proposed ROS-based
system. The system is composed
of two interconnected modules,
an artificial reasoning module
and a CRI-channel module. The
ONO web server has two way of
bidirectional communication: a
websocket and a standard ROS
Subscriber
developed in the server-side application to directly run a
ROS node and communicate with standard ROS publishers
and subscribers.
4 The Robotic Platform ONO
The CRI is implemented through the open source platform
for social robotics (OPSORO),2 which is a promising
and straightforward system developed for face to face
communication composed of a low-cost modular robot
called ONO (see 
[40]. Some of the most important requirements and
characteristics that make ONO interesting for this CRI
strategy are explained in the following sections.
4.1 Appearance and Identity
The robot is covered in foam and also fabric to have a more
inviting and huggable appearance to the children. The robot
has an oversized head to make its facial expressions more
prominent and to highlight the importance for communica-
tion and emotional interaction. As a consequence of its size
and pose, children can interact with the robot at eye height
when the robot is placed on a table.
The robot ONO has not a predefined identity, as the
only element previously conceived is the name. Unlike other
robots that have well-defined identities, such as Probo [9]
or Kaspar [41], in this work the ONO’s identity is built with
the participation of the child through a co-creation process.
For this reason, a neutral appearance is initially used. In the
2Open Source Platform for Social Robotics (OPSORO) 
opsoro.com.
intervention, the therapist can provide the child with clothes
and accessories to define the identity of ONO.
4.2 Mechanics Platform
As the initial design of ONO is composed only of the
actuated face, in this work it was needed to provide the ONO
with some body language. For this purpose, motorized arms
were designed and implemented.
The new design of ONO has a fully face and two
arms actuated, giving a total of 17 Degrees of Freedom
(DOF). The ONO is able to perform facial expressions and
nonverbal cues, such as waving, shake hands and pointing
towards objects, moving its arms (2 DOF x 2), eyes (2 DOF
x 2), eyelids (2 DOF x 2), eyebrows (1 DOF x 2), and mouth
(3 DOF). The robot has also a sound module that allows
explicit positive feedback as well as reinforcement learning
through playing words, conversations and other sounds.
4.3 Social Expressiveness
In order to improve social interaction with a child, the ONO
is able to exhibit different facial expressions. The ONO’s
expressiveness is based on the Facial Action Coding System
(FACS) developed in [42]. Each DOF that composes the
ONO’s face is linked with a set of Action Units (AU) defined
by the FACT, and each facial expression is determined for
specific AU values. The facial expressions are represented as
a 2D vector f e = (v, a) in the emotion circumplex model
defined by valence and arousal [9]. In this context, the basic
facial expressions are specified on a unit circle, where the
neutral expression corresponds to the origin of the space
f e0 = (0, 0). The relation between the DOF position and
the AU values is resolved through a lookup table algorithm
using a predefined configuration file [40].
J Intell Robot Syst (2019) 96:267–281
Content courtesy of Springer Nature, terms of use apply. Rights reserved.

through the open source
platform for social robotics
(OPSORO)
4.4 Adaptability and Reproducibility
The application of the Do-It-Yourself (DIY) concept is
the principal feature of ONO’s design, which facilitates
its dissemination and use in research areas other than
engineering as health care. These characteristics allow ONO
building for any person without specialized engineering
knowledge. Additionally, it is possible to replicate ONO
without the need for high-end components or manufacturing
machines [40]. The electronic system is based on a
Raspberry Pi single-board computer combined with a
custom OPSORO module with circuitry to control up to
32 servos, drive speakers and touch sensors. Any sensor
or actuator compatible with the embedded communication
protocols (UART, I2C, SPI) implemented on the Raspberry
Pi can be used by this platform.
4.5 Control and Autonomy
With the information delivered for the automated reasoning
module, it was possible to automate the ONO’s behavior
and, then, the robot can infer and interpret the children’s
intentions to react most accurately to the action performed
by them, thus enabling a more efficient and dynamic
interaction with ONO. In this work, the automated ONO’s
behavior is partially implemented, i.e., the framework can
modify some physical actions of ONO using the feedback
information about the child’s behavior. The actions suitable
to be modified are gaze shift toward the child in specifics
events, changing from neutral to positive facial expression
when the child looks toward the target, and providing
sound rewards. Also, an Aliveness Behavior Module (ABM)
is implemented to improve the CRI, which consist of
blinking the robot’s eyes and changing its arms among
some predefined poses. Also, the robot can be manually
operated through a remote controller hosted in the client-
side application.
5 Reasoning Module: Machine Learning
Methods for Child’s Face Analysis
The automated child’s face analysis consists of monitoring
nonverbal cues, such as head and body movements, head
pose, eye gaze, visual contact and visual focus of attention.
In this work, a pipeline algorithm is implemented using
machine learning neural models for face analysis. The
chosen methods were developed using state-of-art trained
neural models, available by Dlib3 [43] and OpenFace4
[44]. Some modification such as, turn the neural model an
attribute of the ROS node class and evaluate this in each
topic callback, were needed to run the neural models into a
common ROS node.
The algorithm proposed for child’s face analysis involves
face detection, recognition, segmentation and tracking,
landmarks detection and tracking, head pose, eye gaze and
visual focus of attention (VFOA) estimation. In addition, the
architecture proposed here also implement new methods for
asynchronous matching and fusion of all local data, visual
focus of attention estimation based on Hidden Markov
Model (HMM) and direct connection with the CRI-channel
to influence the robot’s behaviors. A scheme of the pipeline
algorithm is shown in 
3Dlib C++ Library 
4A Open Source Facial Behavior Analysis 
TadasBaltrusaitis/OpenFace.
J Intell Robot Syst (2019) 96:267–281
Content courtesy of Springer Nature, terms of use apply. Rights reserved.

automated child’s face analysis
5.1 Child’s Face Detection and Recognition
The in-clinic setup requires differentiate the child’s face
from other faces detected and found in the scene. For this
reason, a face recognition process was also implemented in
this work. First, the face detection is executed to initialize
the face recognition process and, subsequently, initialize
the landmarks detection. In this work, both detection and
recognition are implemented using deep learning models,
which are described in this section.
In the detection process, a Convolutional Neural Network
(CNN) based face detector with a Max-Margin Object
Detection (MMOD) as loss layer is used [45]. The CNN
consist first of a block composed of three downsampling
layers, which apply convolution with a 5x5 filter size and
2×2 stride to reduce the size of the image up to eight
times its original size and generate a feature map with 16
dimensions. Later, the result are processed for one more
block composed of four convolutional layers to get the final
output of the network. The three first layers of the last
block have 5×5 filter size and 1x1 stride, but, the last layer
has only 1 channel and a 9×9 filter size. The values in
the last channel are large when the network thinks it has
found a face at a particular location. All convolutional block
above are implemented with two additional layers among
convolutional layers, pointwise linear transformation, and
Rectified Linear Units (RELU) to apply the non-saturating
activation function f (x) = max(0, x). The training dataset
used to create the model is composed of 6975 faces and is
available at Dlib’s homepage.5
The face recognition algorithm used in this work is
inspired on the deep residual model from [46]. The
5 face detection dataset-2016-09-30.tar.
gz.
residual network (ResNet) model developed by He et. al
reformulates the convolutional layers to learn a residual
functions F(x) := H(x) − x with reference to the layer
inputs x, instead of learning unreferenced functions. In the
practical implementation, the previous formulation means
inserting shortcut connections, which turn the network into
its counterpart residual version [46]. The CNN model then
transforms each face detected to a 128D vector space in
which images from the same person will be close to each
other, but faces from different people will be far apart.
Finally, the faces are classified as child’s face, caregiver’s
face and therapist’s face.
Both detection and recognition CNN model were
implemented and trained from [43] and released in Dlib
5.2 Face Analysis, Landmarks, Head Pose and Eye
Gaze
This work uses the technique for landmarks detection, head
pose and eye gaze estimation developed by Baltruˇsaitis et
al., named Conditional Local Neural Fields (CLNF) [47].
This technique is an extension of the Constrained Local
Model (CLM) algorithm using specialized local detectors
or patch experts. CNLF model consists of a statistical
shape model, which its learned from data examples and is
parametrized for m components of linear deformation to
control the possible shape variations of the non-rigid objects
[48]. Approaches based on CLM [49, 50] and CLNF [47]
model the object appearance in a local fashion, i.e, each
feature point has its own appearance model to describe the
amount of misalignment.
CLNF-based landmark detection consists of three main
parts: the shape model, the local detectors or patch experts,
and the fitting algorithm, which are detailed below.
J Intell Robot Syst (2019) 96:267–281
Content courtesy of Springer Nature, terms of use apply. Rights reserved.

5.2.1 Shape Model
The CLNF technique uses a linear model to describe non-
rigid deformations called Point Distribution Model (PDM).
The PDM is used to estimate the likelihood of the shapes
being in a specific class, given a set of feature points [48].
This is important for model fitting and shape recognition.
The shape of a face that has n landmark points can be
described as:
X = [X1, X2, . . . , Xn, Y1, Y2, . . . , Yn, Z1, Z2, . . . , Zn],
(1)
and the class that describes a valid instance of a face using
PDM can be represented as:
X = ¯X + �q,
(2)
where
¯X is the mean shape of the face, � described
the principal deformation modes of the shape, and q
represent the non-rigid deformation parameters. Both ¯X
and � are learned automatically from labeled data using
Principal Component Analysis (PCA). The probability
density distribution of the instances into the shape class is
expressed as a zero mean Gaussian with Covariance matrix
� = ([λ1; . . . ; λm]) evaluated at q:
p(q) = N(q; 0; �) =
√(2π)m |�|exp
�
−1
2(qT �−1q)
�
(3)
Once the model is defined, it is necessary to place the 3D
PDM in an image space. The following equation is used
to transform between 3D space to image space using weak
perspective projection [49]:
xi = s · R2D · ( ¯Xi + �iq) + t,
(4)
where ¯Xi
= [¯xi, ¯yi, ¯zi]T is the mean value of the ith
landmark. The instance of the face in an image is, therefore,
controlled using the parameter vector p = [s, w, t, q],
where q represents the local non-rigid deformation, s is a
scaling term, w is the rotation term that controls the 2 × 3
matrix R2D, and t is the translation term.
The global parameters are used to estimate the head
pose in reference to the camera space using orthographic
camera projection and solving the Perspective-n-Point
(PnP) problem respect to the detected landmarks. The PDM
used in [44] was trained on two public datasets [51, 52].
This result in a model with 34 non-rigid (Principal modes)
and 6 rigid shape parameters.
5.2.2 Patch Experts
The patch experts scheme is the main novelty implemented
in the CLNF model. The new Local Neural Field (LNF)
patch expert takes advantage of the non linear relationship
between pixel values and the patch response maps. The LNF
captures two kinds of spatial characteristics between pixels,
such as similarity and sparsity [47].
LNF patch expert can be interpreted as a three layer
perceptron with a sigmoid activation function followed by
a weighted sum of the hidden layers. It is also similar to
the first layer of a Convolutional Neural Network [44].
The new LNF patch expert is able to learn from multiple
illuminations and retain accuracy. This becomes important
when creating landmark detectors and trackers that are
expected to work in unseen environments and on unseen
people.
The learning and inference process is developed using
a gradient-based optimization method to help in finding
locally optimal model parameters faster and more accu-
rately.
In the CLNF model implemented in [44], 28 set in total
of LNF patch experts were trained for seven views and
four scales. The framework uses patch experts specifically
trained to recognize the eyelids, iris and the pupil, in order
to estimate the eye gaze [44].
5.2.3 Fitting Algorithm
For each new image or video frame, the fitting algorithm
of CLNF-based landmark detection process attempts to
find the value of the local and global deformable model
parameters p that minimizes the following function [49]:
E(p) = R(p)
n
�
i=1
Di(xi; I),
(5)
where R is a weight to penalize unlikely shapes, which
depends on the shape model, and D represents the
misalignment of the ith landmark in the image I, which
is function of both the parameters p and the patch experts.
Under the probabilistic point of view, the solution of (5) is
equivalent to maximize the a posteriori probability (MAP)
of the deformable model parameters p:
p
�
p | {li = 1}n
i=1 , I
�
∝ p((p))
n
�
i=1
p (li = 1 | xi, I) ,
(6)
where, li ∈ {1, −1} is a discrete random variable indicating
whether the ith landmark is aligned or misaligned, p(p)
is the prior probability of the deformable parameters p,
and p (li = 1 | xi, I) is the probability of a landmark being
aligned at a particular pixel location xi, which is quantified
from the response maps created by patch. Therefore, the
last term in (6) represents the joint probability of the patch
expert response maps.
The MAP problem is solved using a optimization strategy
designed specifically for CLNF fitting called non-uniform
J Intell Robot Syst (2019) 96:267–281
Content courtesy of Springer Nature, terms of use apply. Rights reserved.

regularized landmark mean shift (NU-RLMS) [47], which
uses two step process. The first step evaluates each of
the patch experts around the current landmark using a
Gaussian Kernel Density Estimator (KDE). The second step
iteratively updates the model parameters to maximize (6).
The NU-RLMS uses expectation maximization algo-
rithm, where the E-step involves evaluating the posterior
probability over the candidates, and the M-step finds the
parameter updated through the mean shift vector v. The
mean shift vector points in the direction where the feature
point should go, but the motion is restricted by the statisti-
cal shape model and the R(p). This interpretation leads to
the new update function:
argmin
�p
�
∥J�p − v∥2
W + r ∥p + �p∥2
˜�−1
�
(7)
where r is a regularization term, J is the Jacobian, which
describe how the landmarks location are changing based
on the infinitesimal changes of the parameters p, ˜�−1 =
diag([0; 0; 0; 0; 0; 0; λ−1
1 ; ... ; λ−1
m ]), and W allows for
weighting of mean-shift vectors. Non-linear least squares
leads to the following update rule:
�p = −
�
J T WJ + r�−1� �
r�−1p − J T Wv
�
(8)
To construct W, the performance of patch experts on training
data is used.
5.3 Data Fusion
The fusion of the local results for the head pose estimation
is done applying a consensus over the rotation algorithm
[53]. This algorithm consists of calculating the weighted
average pose between each camera estimation and its
immediate sensors’ estimation neighbors using the axis-
angle representation. The local pose is penalized by two
weights: the alignment confidence of landmarks detection
procedure and the Mahalanobis distances between the head
pose and a neutral pose.
5.4 Field of View (FoV) and Visual focus of Attention
(VFOA)
The VFOA estimation model is implemented as a dynamic
Bayesian network through a Hidden Markov Model
(HMM). The model assumes a specific set of child’s
attention attractors or targets F. The estimation process
decodes the sequence of child’s head poses Ht
=
(H yaw
t
, H pitch
t
) ∈ R2 in terms of VFOA states Ft ∈ F
at time t [54]. The probability distribution of the head
poses in reference to a given VFOA target is represented
by a Gaussian distribution, whereas the transitions among
these targets are represented by the transition matrix A. The
HMM equations can then be written as follows:
P (Ht | Ft = f, μh
t ) = N(Ht| μh
t (f ), �H(f ))
(9)
p(Ft = f | Ft−1 = ˆf ) = Af ˆf .
(10)
The Gaussian covariances is defined manually to reflect
target sizes and head pose estimation variability. Moreover,
the Gaussian means corresponding to each specific target
μh
t
is calculated through a gaze model that sets this
parameter as a fixed linear combination of the target
direction and the head reference direction [55]:
μh
t (f ) = α ⋆ μt(f ) + (12 − α) ⋆ Rt,
(11)
where ⋆ denotes the component wise product 12 = (1, 1),
α = (αyaw, αpitch) = (0.7, 0.5) are adjustable constants
that describe the fraction of the gaze shift that corresponds
to the child’s head rotation, μt ∈ (R2)K is the directions
of the given K targets, and Rt
∈
R2 represents the
reference direction, which is the average head pose over
a time window W R. The above assumption describes the
body orientation behavior of any child who tends to orient
himself/herself towards the set of gaze targets to make more
comfortable to rotate his/her head towards different targets
Rt =
W R
t�
i=t−W R
Hi.
(12)
Finally, for the estimation of the VFOA sequence a classic
Viterbi algorithm of HMM is implemented [54].
6 Case Study
For the case study, the vision system is composed of
three Kinect V2 sensors. Each sensor is connected to a
workstation equipped with a processor of Intel Core i5
family and a GeForce GTX GPU board (two workstation
with GTX960 board, and one workstation with GTX580
board). All workstation are connected through a local area
network synchronized using the NTP protocol.6 The sensors
were intrinsically and extrinsically calibrated through a
conventional calibration process using a standard black-
white chessboard.7
6.1 In-clinic Setup
A multidisciplinary team of psychologists, doctors and
engineers developed a case study using a psychology room
equipped with a unidirectional mirror to perform behavioral
6Network Time Protocol Homepage, 
7Tools for using the Kinect One (Kinect V2) in ROS, 
com/code-iai/iai kinect2.
J Intell Robot Syst (2019) 96:267–281
Content courtesy of Springer Nature, terms of use apply. Rights reserved.

interventions room of in-clinic
setup
observation appropriately. The room was prepared with a
table and three chairs: one for the child, another for the
caregiver and a third one for the therapist. The robot was
placed on the table, and the following toys, a helicopter,
a truck and a train, were attached to room’s walls. The
RGBD sensors were located close to the walls, and no
additional camera was placed on the robot or the table, so
as not to attract the child’s attention. A representation of the
interventions room of in-clinic setup is shown in 
6.2 Intervention Protocol
In this work, a technology-based system was used as a
tool in various stages of the ASD diagnostic process.
The framework can be implemented to extract different
behavioral features to be assessed, e.g., eye contact,
stereotyped movements of the head, concentration and
excessive interest in objects or events. However, for the
scope of this research, a specific clinical setup intervention
to assess Joint Attention (JA) behaviors is presented. The
intervention aims to evaluate the capacity of JA; which can
be divided into three classes: initiation of joint attention
(IJA), responding to joint attention bids (RJA), and initiation
of request behavior (IRB) [6]. The therapist guides the
intervention all the time and leverages the robot device as
an alternative channel of communication with the child,
for the above, both the specialist and the robot remained
in the room during the intervention. The children were
accompanied throughout the session by a caregiver who
was oriented not to help the child in the execution of the

cues elicited by the CRI, to look
towards the therapist, towards
the robot, point and self
occlusion
J Intell Robot Syst (2019) 96:267–281
Content courtesy of Springer Nature, terms of use apply. Rights reserved.

child’s face analysis pipeline for
the case study. Face detection
and recognition, landmarks
detection, head pose and eye
gaze estimation were executed
tasks. The exercise developed aimed to direct the attention
of the child towards objects located in the room through
stimuli, such as, look at, point and speak. The stimuli were
generated first only by the therapist and later just by the
robot.
6.3 Subjects
Three children without confirmed ASD diagnosis, but with
evidence of risk factors, and three typically developing
(TD) children as the control group participated in the
experiments. All volunteers participated with their parent’s
consent, which were five boys (3 ASD, 2 TD) and
one TD girl, between 36 months to 48 months. Each
volunteer participated in one single session. The goal was to
analyze the based-line of the child’s behavior and establish
differences in the behavioral reaction between TD and ASD
children for stimuli generated through CRI and leverage the
novelty effect raised by the robot mediator.
7 Results and Discussion
The child’s nonverbal cues elicited by the CRI can be
observed in 
tagged to perform the behavioral coding are shown in the
six pictures. The tagged behaviors were: to look towards
an object, towards the robot, and towards the therapist, to
point and, to respond to a prompt of both mediators and self
occlusion. Typical occlusion problem, as occlusion by hair,
hands and the robot were detected.
The performance of video processing in the proof of
concept session is reported in 
sessions, the child’s face detection and recognition, the

the child’s head/neck rotation
(yaw rotation) for a TD group
J Intell Robot Syst (2019) 96:267–281
Content courtesy of Springer Nature, terms of use apply. Rights reserved.

landmarks detection, head pose and eye gaze estimation for
different viewpoints are shown in 
process was able to detect all faces in the session
successfully in most cases.
The child’s head pose was captured throughout the
session and analyzed automatically to estimate the evolution
over time of child’s head and the VFOA. Along the
session, the child’s neck right/left rotation movement was
predominant (Yaw axis), while the neck flexion/extension
(Pitch axis) and neck R/L lateral flexion movements (Roll
axis) remained approximately constant. The Yaw rotation of
the TD children group is reported in 
blue stripe indicates the intervention period with therapist-
mediator, and the vertical light green stripe represents
the period with robot-mediator. The continuous blue line
represents the raw data recorded, and the continuous red line
describes the average data trend. From the observation of the
three plot, the TD children started the intervention looking
towards the robot, evidently, the robot was a naturalistic
attention attractor. Subsequently, when the therapist begins
the protocol explaining the tasks, the children attention
shifts towards the therapist. The children remained this
behavior until that the therapist introduced the robot-
mediator. In this transition, the children’s behaviors, such as,
RJA and IJA toward the therapist were observed. Once the
therapist changed the mediation with the robot, the children
turned his/her attention to the robot and the objects in the
room.
A more detailed analysis of one of the TD volunteers is
shown in 
session; the plot (B) and plot (C) are a zoom of the period
with therapist and robot mediator, respectively. The colors
convention in the three plots of 
generated by the automated estimation of VFOA. From
these scenarios, some essential aspects already emerge. In
the therapist-mediator interval, the child responded to JA
task using only one repetition for all prompt level. The
child’s behavior of RJA was according to the protocol,
i.e., the child looked towards the therapist to wait for
instructions, rapidly the child searched in the target, and
next looked again toward the therapist (Color sequence:
light blue - yellow - light blue - orange - light blue - red).
This behavior was the same for all prompts. In contrast,
with the robot-mediator, the child did not look toward
the robot among indications at consecutive targets (Color
sequence: light green - yellow - orange - red - orange -
yellow). The above happened because, in the protocol, both
mediators executed the instructions in the same order, and
J Intell Robot Syst (2019) 96:267–281
Content courtesy of Springer Nature, terms of use apply. Rights reserved.

the child memorized the commands and the object’s position
until the robot mediator interval. This fact did not affect
the intervention’s aim, as the robot mediator succeeded to
elicit the child’s behaviors of RJA and IJA. In addition,
as highlighted in the plot (A) in 
finalized and the robot mediator said goodbye, again, RJA
and IJA behaviors were perceived. The pictures (a-d) show
these events: first the child said goodbye towards the robot,
then, he looked the therapist to confirm that the session
ended and looked again towards the robot, finally the child
took the robot’s hand.
From the analysis of the three TD volunteers, the same
reported behaviors were perceived. However, the analysis
of the children in the ASD group showed different behavior
patterns concerning comfort, visual contact and novelty
stimulus effect during the sessions. The evolution over time
of the child’s head/neck rotation (yaw rotation) for an ASD
group is shown in 
in the ASD group maintained more visual contact with
the robot compared to the therapist and exhibited more
interest in the robot platform compared to the TD children.
However, the performance of the children in the activities
of JA did not improve significantly when the robot executed
the prompt. On the other hand, the clinicians manifested that
in all cases the first visual contact toward them occurred
in the instant that the robot entered the scene and started
interacting, i.e., the ONO mediation elicited behaviors of
IJA towards the therapist. In addition, the CwASD exhibited
less discomfort regarding the session, from the first moment
when the robot initiated mediation in the room and, in some
cases, when showed appearance of verbal and non-verbal
pro-social behaviors. These facts did not arise with the TD
children, because the first visual contact with the therapist
occurred when they entered the room. Additionally, TD
children showed the ability to divide the attention between
the robot and the therapist from the beginning to the end
of the intervention, exhibiting comfort in every moment.
The behavior modulation of CwASD is observed in 
Before the period with robot-mediator the children exhibited
discomfort (unstable movements of their head), and after
of this period, the head movement tended to be more
stable.
The novelty of a robot-mediator at diagnostic session
can be analyzed as an additional stimulus of the CRI.
Accordingly, in this case study the children of the ASD
group showed more behavior modification (attention and
comfort) produced by the robot interaction at the beginning
of the CRI, remaining until the end of the session. On the
other hand, the children of the TD group responded to the
novelty effect of the robot mediator from the time the child
entered the room and saw the robot, until the beginning of
the therapist presentation. For the above, despite the novelty
J Intell Robot Syst (2019) 96:267–281
Content courtesy of Springer Nature, terms of use apply. Rights reserved.

of the stimuli effect, these did not seem to affect the social
interaction between the TD children and the therapist, and
in contrast, these stimuli seemed to enhance the CwASD
social interaction with the therapist along the intervention.
These results are impressive, since they show the
potential of CRI intervention to systematically elicit
differences between the pattern of behavior on TD and ASD
children. We identified RJA and IJA toward the therapist at
the beginning of the intervention, at the transition between
therapist to robot mediator, and at the end for all TD
children. In contrast, we only identified IJA towards the
therapist in the transition between mediators, for ASD
children. This fact shows a clear difference of behavior
pattern between CwASD and TD children, which can be
analyzed using a JA task protocol. In fact, these pattern
differences can be used as evidence to improve the ASD
diagnosis.
8 Conclusions
This work presented a Robot-Assisted tool to assist and
enhance the traditional practice of ASD diagnosis. The
designed framework combines a vision system with the
automated analysis of nonverbal cues in addition to a
robotic platform; both developed upon open source projects.
This research contributes to the state-of-the-art with an
innovative flexible and scalable architecture capable to
automatically register events of joint attention and patterns
of visual contact before and after of a robot-based mediation
as well as the pattern of behavior related to comfort or
discomfort along the ASD intervention.
In addition, an artificial vision pipeline based on a multi-
camera approach was proposed. The vision system performs
face detection, recognition and tracking, landmark detection
and tracking, head pose, gaze and estimation of visual focus
of attention was proposed, with its performance considered
suitable for use into conventional ASD intervention. At
least one camera captured the child’s face in each sample
frame. Furthermore, the feedback information about the
child’s performance was successfully used to modulate the
supervised behavior of ONO, improving the performance of
the CRI and the visual attention of the children. Regarding
the VFOA estimation, the algorithm was able to estimate
the target into the FoV in different situations recurrently.
Also, the robot was able to react according to the estimation.
However, the algorithm only failed when occlusion by the
child’s hands is generated. On the other hand, the occlusion
by the therapist and the robot was compensated using the
multi-camera approach. The child’s face recognition system
showed to be imperative to analyze the child’s behavior in
the clinical setup implemented in this work, which required
the caregiver’s attention in the room.
Despite the limited number of children of this study,
preliminary results of this case study showed the feasibility
of identifying and quantify differences in the patterns of
behavior of TD children and CwASD elicited by the CRI
intervention. Through the proof of concept, it is evidenced
here the system ability to improve the traditional tools used
in ASD diagnosis. As future works, it is recommended a
study to replicate the protocol proposed in this paper with
ten CwASD and ten TD children. Another suggestion is to
quantify other kinds of behaviors in addition to that assessed
in this paper, such as verbal utterance patterns, physical
and emotional engagement, object or event preferences and
gather more evidence to improve the assistance to therapists
in ASD diagnosis processes.