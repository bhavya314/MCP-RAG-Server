Introduction
Autism spectrum disorder (ASD) can be reliably diagnosed
as early as 24 months old and the risk signs can be
detected
as
early
as
6–12 months
old
[Dawson
&
Bernier, 2013; ]. Despite this, the aver-
age age of diagnosis in the United States remains around
4 years of age . While there is
mixed evidence for the stability of autism traits over early
childhood
[Bieleninik
et
al.,
Waizbard-Bartov
et al., 2020], the delay in diagnosis can still impact timely
intervention during a critical window of development. In
response to this, in 2007 the American Academy of Pedi-
atrics published guidelines supporting the need for all
children to be screened for ASD between 18- and
24-months of age as part of their well-child visits [Myers,
Johnson, & Council on Children With Disabilities, 2007].
Current screening typically relies on caregiver report,
such as the Modiﬁed Checklist of ASD in Toddlers—
Revised
with
Follow-up
(M-CHAT-R/F)
(Robins
et al., 2014). Evidence suggests that a two-tiered screen-
ing approach, including direct observational assessment
of the child, improves the positive predictive value of
M-CHAT
screening
by
[Khowaja,
Robins,
&
From the Duke Center for Autism and Brain Development, Department of Psychiatry and Behavioral Sciences, Duke University School of Medicine, Dur-
ham, North Carolina, USA (K.L.H.C., J.H., K.C., H.L.E., S.E., S.V., G.D.); Department of Electrical and Computer Engineering, Duke University, Durham,
North Carolina, USA (J.H., S.E.); Department of Pediatrics, University of Utah, Salt Lake City, Utah, USA (K.C.); Department of Population Health Sci-
ences, Duke University School of Medicine, Durham, North Carolina, USA (S.J.L.); Department of Pediatrics, Duke University School of Medicine, Dur-
ham, North Carolina, USA (J.P.B.); NYU Langone Child Study Center, New York University, New York, New York, USA (H.L.E.); Departments of
Biomedical Engineering Computer Science, and Mathematics, Duke University, Durham, North Carolina, USA (G.S.); Duke Institute for Brain Sciences,
Duke University, Durham, North Carolina, USA (G.D.)
Received April 7, 2020; accepted for publication August 24, 2020
Address for correspondence and reprints: Kimberly L. H. Carpenter, Duke Center for Autism and Brain Development, Department of Psychiatry and
Behavioral Sciences, Duke University School of Medicine, 2608 Erwin Rd #300, Durham, NC 27705. E-mail: 
Published online 00 Month 2020 in Wiley Online Library (wileyonlinelibrary.com)
© 2020 International Society for Autism Research and Wiley Periodicals LLC
INSAR
Autism Research 000: 1–12, 2020

Adamson, 2017] and may reduce ethnic/racial disparities
in general screening . Current tools
for observational assessment of ASD signs in infants and
toddlers, such as the Autism Observation Scale for Infants
(AOSI) and Autism Diagnostic Observation Schedule
(ADOS), take substantial time and training to administer,
resulting in a shortage of qualiﬁed diagnosticians to per-
form these observational assessments. As such, there
remains a critical need to develop feasible, scalable, and
reliable tools that can characterize ASD risk behaviors
and identify those children who are most in need of
follow-up by an ASD specialist. In an effort to address this
critical need, we have embarked on a program of research
using computer vision analysis (CVA) to develop tools for
digitally phenotyping early emerging risk behaviors for
ASD [Dawson & Sapiro, 2019]. If successful, such digital
screening tools have the opportunity to help existing
practitioners reach more children and assist in triaging
boundary cases for review by specialists.
One
of
the
early
emerging
signs
of
ASD
is
a
tendency to more often display a neutral facial expres-
sion. This pattern is evident in the quality of facial
expressions
and
in
sharing
emotional
expressions
with
others
[Adrien
et
al.,
Baranek,
S.
Clifford
&
Dissanayake,
S.
Clifford,
Young,
&
Williamson,
S.
M.
Clifford
&
Dissanayake, 2008; ; Osterling, Daw-
son, & Munson, 2002; Werner, Dawson, Osterling, &
Dinno, 2000]. A restricted range of emotional expres-
sion and its integration with eye gaze (e.g., during social
referencing) have been found to differentiate children
with ASD from typically developing children, as well
as those who have other developmental delays, as
early
as
12 months
of
age
[Adrien
et
al.,
S. ; ; Gangi,
Ibanez, & Messinger, 2014; Nichols, Ibanez, Foss-Feig, &
Stone, 2014]. While core features of ASD vary by age,
cognitive ability, and language, one of the most stable
symptoms from early childhood through adolescences
is increased frequency of neutral expression [Bal, Kim,
Fok, & Lord, 2019]. As such, differences in facial affect
may show utility in assessing early risk for ASD.
A recent meta-analysis of facial expression production
in autism found that individuals with ASD display facial
expressions less often than non-ASD participants and
that, when they did display facial expressions, the expres-
sions occurred for shorter durations and were of different
quality than non-ASD individuals (Trevisan, Hoskyn, &
Birmingham, 2018). Decreases in the frequency of both
emotional facial expressions and the sharing of those
expressions with others has been demonstrated across
naturalistic interactions [Bieberich &
Morgan, 2004;
Czapinski
&
Bryson,
Dawson,
Hill,
Spencer,
Galpert,
&
Watson,
Mcgee,
Feldman,
&
Chernin, 1991; Snow, Hertzig, & Shapiro, 1987; Tantam,
Holmes, & Cordess, 1993], in lab-based assessments such
as during the ADOS or the AOSI
 and in response to emotion-eliciting
videos [Trevisan, Bowering, & Birmingham, 2016]. Fur-
thermore, higher frequency of neutral expressions corre-
lates with social impairment in children with ASD
 and differentiates them from chil-
dren with other delays [Bieberich & Morgan, 2004;
Yirmiya, Kasari, Sigman, & Mundy, 1989]. As such, fre-
quency and duration of facial affect is a promising early
risk marker for young children with autism.
Previous research on atypical facial expressions in chil-
dren with ASD has relied on hand coding of facial expres-
sions,
which
is
time
intensive
and
often
requires
signiﬁcant training [Bieberich & Morgan, 2004; S. Clifford
et al., 2007; ; ;
Mcgee
et
al.,
Nichols
et
al.,
Snow
et al., 1987]. This approach is not scalable for use in gen-
eral ASD risk screening or as a behavioral biomarker or
outcome assessment for use in large clinical trials. As
such, the ﬁeld has moved toward automating the coding
of facial expressions. In one of the earliest studies of this
approach, Guha and colleagues demonstrated that chil-
dren with ASD have atypical facial expressions when
mimicking others. However, their technology required
the children to wear markers on their face for data cap-
ture [Guha, Yang, Grossman, & Narayanan, 2018; Guha
et al., 2015], which is both invasive and not scalable.
More recently, several groups have applied non-invasive
CVA technology to measuring affect in older children
and adults with ASD within the laboratory setting
[Capriola-; ; Samad
et al., 2018]. This represents an important move toward
scalability as CVA approaches do not rely on the presence
of physical markers on the face to extract emotion infor-
mation. Rather, CVA relies on videos of the individual in
which features around speciﬁc regions on a face (e.g., the
mouth and eyes) are extracted. Notably, these features
mirror those used by the manually rated facial affect cod-
ing system (FACS) [Ekman, 1997]. Both our earlier work
 and that of others [Capriola-Hall
et al., 2019] have shown good concordance between
human coding and CVA rating of facial emotions. Fur-
thermore, previous research in adults has demonstrated
that CVA can detect neutral facial expressions more reli-
ably than human coders [Lewinski, 2015].
Building on previous work applying CVA in laboratory
settings, we have developed a portable tablet-based tech-
nology that uses the embedded camera and automatic
CVA to code ASD risk behaviors in <10 min across a range
of non-laboratory settings (e.g., pediatric clinics, at home,
etc.). We developed a series of movies designed to capture
children’s attention, elicit emotion in response to novel
and interesting events, and assess the toddler’s ability to
sustain attention and share it with others. By embedding
INSAR
Carpenter et al./Digital behavioral phenotyping in ASD

these movies in a fully automated system on a cost-
effective tablet whereby the elicited behaviors, in this
case the frequency of different patterns of facial affect,
are automatically encoded with CVA, we aim to create a
tool that is objective, efﬁcient, and accessible. The cur-
rent analysis focuses on preliminary results supporting
the utility of this tablet-based assessment for the detec-
tion of facial movement and affect in young children and
the use of facial affect to differentiate children with and
without ASD. Though facial affect is the focus of the cur-
rent analysis, the ultimate goal is to combine information
across autism risk features collected through the current
digital screening tool [e.g., delayed response to name as
described in ], to develop a risk score
based on multiple behaviors [Dawson & Sapiro, 2019].
This information could then be combined with addi-
tional measures of risk to enhance screening for ASD.
Methods
Participants
Participants were 104 children 16–31 months of age
(
mary care visit by a research assistant embedded within
the clinic or via referral from their physician, as well as
through community advertisement (N = 4 in the non-
ASD group and N = 15 in the ASD group). For children
recruited
within
the
pediatric
clinics,
recruitment
occurred at the 18- or 24-month well-child visit at the
same time as they received standard screening for ASD
with the M-CHAT-R/F. A total of 76% of the participants
recruited in the clinic by a research assistant chose to par-
ticipate. Of the participants who chose not to participate,
11% indicated that they were not interested in the study,
whereas the remainder declined due to not having
enough time, having another child to take care of, want-
ing to discuss with their partner, or their child was
already too distressed after the physician visit. All chil-
dren who enrolled in the study found the procedure
engaging enough that they were able to provide adequate
data for analysis. Because the administration is very brief
and non-demanding, data loss was not a signiﬁcant
problem.
Exclusionary criteria included known vision or hearing
deﬁcits, lack of exposure to English at home, or insufﬁ-
cient English language skills for caregiver’s informed con-
sent. Twenty-two children were diagnosed with ASD. The
non-ASD comparison group (N = 82) was comprised of
74 typically developing children and 8 children with a
non-ASD delay, which was deﬁned by a diagnosis of lan-
guage delay or developmental delay of clinical signiﬁ-
cance sufﬁcient to qualify for speech or developmental
therapy as recorded in the electronic medical record. All
caregivers/legal guardians gave written informed consent,
and the study protocol was approved by the Duke Uni-
versity Health System IRB.
Children recruited from the pediatric primary care
settings received screening with a digital version of the
M-CHAT-R/F as part of a quality improvement study
ongoing in the clinic . Participants
recruited from the community received ASD screening
with the digital M-CHAT-R/F prior to the tablet assess-
ment. As part of their participation in the study, children
who either failed the M-CHAT-R/F or for whom there was
caregiver or physician concern about possible ASD under-
went diagnostic testing using the ADOS-Toddler (ADOS-
T) conducted by a licensed psycholo-
gist or research-reliable examiner supervised by a licensed

Sample Demographics
Typically developing (N = 74; 71%)
Non-ASD delay (N = 8; 8%)
ASD (N = 22; 21%)
Age
Months [mean (SD)]
Sex
Female
3 (38)
5 (23)
Male
5 (62)
Ethnicity/race
African American
1 (13)
3 (14)
Caucasian
2 (25)
Hispanic
1 (1)
0 (0)
1 (4)
Other/unknown
5 (62)
8 (37)
Insurancea
Medicaid
1 (14)
6 (67)
Non-Medicaid
6 (86)
3 (33)
MCHAT resultb
Positive
1 (1)
0 (0)
Negative
4 (18)
aInsurance status was unknown for 17 (16%) of participants in this study.
bChildren for whom the MCHAT was negative but received and ASD diagnosis were referred for assessment due to concerns by either the parent or the
child’s physician.
INSAR
Carpenter et al./Digital behavioral phenotyping in ASD

psychologist.
The
mean
ADOS-T
score
was
(SD = 4.67). A subset of the ASD children (N = 13) also
received
the
Mullen
Scales
of
Early
Learning
(Mullen, 1995). The mean IQ based on the Early Learning
Composite
Score
for
this
subgroup
was
(SD = 25.95). None of the children in the non-ASD com-
parison group was administered the ADOS or Mullen.
Children’s demographic information was extracted
from the child’s medical record or self-reported by the
caregiver at their study visit. Children in the ASD group
were, on average, 4 months younger than the compari-
son group (t[102] = 4.64, P < 0.0001). Furthermore, as
would be expected, there was a higher proportion of
males in the ASD group than in the comparison group,
though the difference was not statistically signiﬁcant (χ2
[1, 104] = 2.60, P = 0.11). There were no differences in the
proportion of racial/ethnic minority children between
the two groups (χ2 [1, 104] = 1.20, P = 0.27). When
looking only at the children for which Medicaid status
was known, there was no difference in the proportion of
children on Medicaid in the ASD and the non-ASD group
(χ2 [1, 87] = 1.82, P = 0.18).
Stimuli and Procedure
A series of developmentally appropriate brief movies
designed to elicit affect and engage the child’s attention
were shown on a tablet while the child sat on a care-
giver’s lap. The tablet was placed on a stand approxi-
mately 3 ft away from the child to prevent the child from
touching the screen as depicted in previous publications
[; ; Hashemi
et al., 2015; ]. Movies consisted of
cascading
bubbles
(2 × 30 sec),
a
mechanical
bunny
(66 sec), animal puppets interacting with each other
(68 sec), and a split screen showing a woman singing
nursery rhymes on one side and dynamic, noise-making
toys on the other side (60 sec; 
included stimuli that have been used in previous studies
of ASD symptomatology , as well as
developed speciﬁcally for the current tablet-based tech-
nology to elicit autism symptoms, based on Dawson
et
al.
Jones,
Dawson,
Kelly,
Estes,
and
Webb [2017], Jones et al. [2016], and Luyster et al. [2009].
At three points during the movies, the examiner located
behind the child called the child’s name.
Prior to the administration of the app, caregivers were
clearly instructed not to direct their child’s attention or
in any way try to inﬂuence the child’s behavior during
the assessment. Furthermore, if the caregiver began to try
to direct their child’s attention, the examiner in the room
immediately asked the caregiver to refrain from doing
so. If the caregiver persisted, this was noted on our valid-
ity form and the administration would have been consid-
ered
invalid.
Researchers
stopped
the
task
for
one
comparison
participant
due
to
crying.
Researchers
restarted the task for three participants with ASD due to
difﬁculty remaining in view of the tablet’s camera for
more than half of the ﬁrst video stimulus. If other family
members were present during the well-child visit, they
were asked to stand behind the caregiver and child so as
to not distract the child during the assessment. Addition-
ally, for children assessed during a well-child visit,
research assistants were instructed to collect data prior to
any planned shots or blood draws.
Computer Vision Analysis
The frontal camera in the tablet recorded video through-
out
the
experiment
at
resolution
and
30 frames per second. The CVA algorithm [Hashemi
et al., 2018] ﬁrst automatically detected and tracked
49 facial landmarks on the child’s face [De la Torre
et al., 2015]. Head positions relative to the camera were
estimated by computing the optimal rotation parameters
between the detected landmarks and a 3D canonical face
model [Fischler & Bolles, 1981]. A “not visible” tag was
assigned to frames where the face was not detected or the
face
exhibited
drastic
yaw
(>45�
from
center).
We
acknowledge that the current method used only indicates
whether the child is oriented toward the stimulus and
does not track eye movements. For each “visible” frame,
the probability of expressing three standard categories of
facial expressions, Positive, Neutral (i.e., no active facial

Example of movie stimuli. Developmentally appropriate movies consisted of cascading bubbles, a mechanical bunny, animal
puppets interacting with each other, and a split screen showing a woman singing nursery rhymes on one side and dynamic, noise-making
toys on the other side.
INSAR
Carpenter et al./Digital behavioral phenotyping in ASD

action
unit),
or
Other
(all
other
expressions),
was
assigned . The model for automatic
facial expression is an extension of the pose-invariant
and cross-modal dictionary learning approach originally
described in Hashemi et al. [2015]. During training, the
dictionary representation is setup to map facial informa-
tion between 2D and 3D modalities and is then able to
infer discriminative facial information for facial expres-
sions recognition even when only 2D facial images are
available at deployment. For training, data from Bing-
hamton University 3D Facial Expression database [Yin,
Wei, Sun, Wang, & Rosato, 2006] were used, along with
synthesized faces images with varying poses [see Hashemi
et al., 2015 for synthesis details]. Extracted image features
and distances between a subset of facial landmarks were
used as facial features to learn the robust dictionary.
Lastly, using the inferred discriminative 3D and frontal
2D facial features, a multiclass support vector machine
[Chang & Lin, 2011] was trained to classify the different
facial expressions.
In
recent
years,
there
has
been
progress
on
automatic facial expression analysis of both children
and
toddlers
[Dys
&
Malti,
Gadea,
Aliño,
Espert,
&
Salvador,
Haines
et
al.,
LoBue & Thrasher, 2014; Messinger, Mahoor, Chow,
& Cohn, 2009]. In addition to this, we have previously
validated our CVA algorithm against expert human rater
coding of facial affect in a subsample of 99 video record-
ings across 33 participants (ASD = 15, non-ASD = 18).
This represents 20% of the non-ASD sample and a mat-
ched group from the ASD sample. The selection of partici-
pants for this previously published validation study was
based on age distribution to ensure representation across
the range of ages for both non-ASD and ASD groups. This
previous work showed strong concordance between CVA-
and human-rated coding of facial emotion in this data
set, with high precision, recall, and F1 scores of 0.89,
0.90, and 0.89, respectively .
Statistical Approach
For each video frame, the CVA algorithm produces a
probability value for each expression (Positive, Neutral,
Other). We calculated the mean of the probability values
for each of the three expression types within non-
overlapping 90-frame (3 sec) intervals, excluding frames
when the face was not visible. A 3-sec window was
selected as it provided us with a continuous distribution
of the emotion probabilities, while still being within the
0.5–4-sec window of a macroexpression (Ekman, 2003).
Additionally, for each of the name call events we
removed the time window starting at cue for the name
call prompt through the point where 75% of the audible
name calls actually occurred, plus 150 frames (5 sec). This
window
was
selected
based
on
our
previous
study
 showing that orienting tended to
occur within a few seconds after a name call. We calcu-
lated the proportion of frames the child was not attend-
ing to the movie stimulus, based on the “visible” and
“not visible” tags described above, within each 90-frame
interval, excluding name call periods. Thus, for each
child, we generated four variables (mean probabilities of
Positive, Neutral, or Other; and proportion of frames not
attending) for each 90-frame interval within each of the
ﬁve movies.
To evaluate differences between ASD and non-ASD
children at regular intervals throughout the assessment,
we ﬁt a series of bivariate logistic regressions to obtain
the odds ratios for the associations between the mean
expression probability or attending proportion during a
given interval, parameterized as increments of 10%
points, and ASD diagnosis. We then ﬁt a series of multi-
variable logistic regression models, separately for each
movie and variable, which included parameters for each
of the 3-sec intervals within the movie to predict ASD
diagnosis. Given the large number of intervals relative to
the small sample size, we used a Least Absolute Shrinkage
and Selection Operator (LASSO) penalized regression
approach [Tibshirani, 1996] to select a parsimonious set
of parameters representing the intervals within each
movie and expression type that were most predictive of
ASD diagnosis.
For each of the ﬁve movies, we then combined the
LASSO-selected interval parameters into a full logistic
model. When more than one expression parameter was
selected for a given interval, we selected the one with the
stronger odds ratio estimate. Analyses were conducted
with and without age as a covariate. Since the small study
size precluded having separate training and validation
sets, we used leave-one-out cross-validation to assess
model
performance.
Receiver-operating
characteristics
(ROC) curves were plotted and the c-statistic for the area
under the ROC curve was calculated for each movie.
Results

depicts
the
odds
ratio
analysis
using
the
“Rhymes and Toys” movie as one illustrative example. As
shown by the variability in the odds ratio estimates, some
parts of the movies elicited strongly differential responses
in certain patterns of expression (blue window, 
while in other sections, there were not substantial differ-
ences between the two groups (green window, 
Overlaid on the plot are the odds ratios and conﬁdence
bands
for
the
interval
parameters
selected
by
the
expression-speciﬁc LASSO models. These selected param-
eters were then used in the movie-level logistic models
for which we calculated classiﬁcation metrics.
INSAR
Carpenter et al./Digital behavioral phenotyping in ASD

movie-level logistic models after leave-one-out cross-
validation. ROC curves analyses were performed for each
video individually. The model for the “Rhymes” movie
yielded the strongest predictive ability, with an area
under the curve (AUC) of 0.73 (95% conﬁdence interval
[CI] 0.59–0.86), followed by the “Puppets” (AUC = 0.67;
95% CI 0.53–0.80) and the “Bunny” (AUC = 0.66; 95%
CI 0.51–0.82) videos. Finally, the two “Bubbles” movies
that bookend the stimulus sets were the least predictive,
with AUCs of 0.62 (95% CI 0.49–0.74) and 0.64 (95% CI
0.51–0.76), respectively. Because there was a signiﬁcant
difference in age between the ASD and non-ASD com-
parison groups, we ran a second set of ROC analyses
where age was included as a covariate, shown in 
Results remained signiﬁcant after including the age
covariate.
Given the preponderance for the Other emotional cate-
gory in our non-ASD comparison group, we explored:
(a) what speciﬁc facial movements are driving this cate-
gory of Other expressions and (b) how it differs from the
Neutral expression category. We focused on analyzing
movements of facial landmarks and head pose angles and
how they differ between the facial expression categories.
Our CVA algorithm aligns the facial landmarks of the
child to a canonical face model through an afﬁne trans-
formation, which normalizes the landmark locations
across all video frames to a common space. This normali-
zation process is commonly used across CVA tasks related
to facial analysis as it allows one to analyze/compare
landmark locations across different frames or partici-
pants. With this alignment step, we were able to quantify
the distances between the eye corners and the corners of
the eyebrows, the vertical distance between the inner lip
points, and the vertical distance between the outer lip
points (
ated the Neutral from the Other facial expression cate-
gory,
we
assessed
differences
between
these
facial
landmark distances of a given child when they were pre-
dominately expressing Neutral versus Other facial expres-
sions. We also included yaw and pitch head pose angles
since they may play a role in the alignment process.

Time series of odds ratios for the association between the mean expression probability or proportion attending and ASD
diagnosis. Using the “Rhymes” movie as one illustrative example, lines depict the odds of meeting criteria for ASD (OR > 1) or being in
the non-ASD comparison group (OR < 1) for each of the outcomes of interest for each 3 sec time bin across the movie. Points with error
bars are intervals that were selected by the LASSO regression models and included in the ﬁnal logistic model. The blue window depicts a
segment of the movie where there were differential emotional responses between the ASD and non-ASD children. The green window
depicts a segment of the movie in which there was no difference in emotional responses between the groups.

Receiver-operating characteristics (ROC) curves. ROC
curves were calculated for predictive ability of expression-speciﬁc
LASSO selected interval parameters for facial expressions and
attention to stimulus for each movie independently.
INSAR
Carpenter et al./Digital behavioral phenotyping in ASD

We focused in on three stimuli in which participants
exhibited
high
probabilities
of
Other
expressions,
namely, the ﬁrst Bubbles, Puppets, and Rhymes and Toys
videos. Out of the 104 participants, all exhibited frames
where both Neutral and Other facial expressions were
dominant (probability of expression over 60%). A Wilson
signed-ranks
test,
reported
as
(median
difference,
P value), indicated that within individual participants for
each diagnostic group, the median differences between
the distances of Other versus Neutral facial expressions
were signiﬁcantly higher for inner right eyebrows (non-
ASD: diff = 2.5, P < 8.1e-10; ASD: diff = 4.0, P < 1.1e-4),
inner left eyebrows (non-ASD: diff = 2.4, P < 1.5e-9; ASD:
diff = 3.6, P < 1.4e-4), outer right eyebrows (non-ASD:
diff = 1.2, P < 1.0e-5; ASD: diff = 2.3, P < 1.6e-4), outer left
eyebrows (non-ASD: diff = 0.9, P < 3.4e-6; ASD: diff = 1.4,
P < 1.4e-3), and mouth height (non-ASD: diff = 1.5,
P < 1.0e-3), as well as for pitch head pose angles (non-
ASD: diff = 2.8, P < 8.7e-10; ASD: diff = 4.4, P < 1.2e-3);
but not for eye heights, lips parting, nor yaw head pose
angles.
Discussion
The present study evaluated an application administered
on a tablet that was comprised of carefully designed
movies that elicited affective expressions combined with
CVA of recorded behavioral responses to identify patterns
of facial movement and emotional expression that differ-
entiate toddlers with ASD from those without ASD. We
demonstrated that the movies elicited a range of affective
facial expressions in both groups. Furthermore, using
CVA we found children with ASD were more likely to dis-
play a neutral expression than children without ASD
when watching this series of videos, and the patterns of
facial expressions elicited during speciﬁc parts of the
movies differed between the two groups. We believe this
ﬁnding has strong face validity that rests on both
research and clinical observations of a restricted range of
facial expression in children with autism. Furthermore,
this replicates a previous ﬁnding of our group reporting
increased
frequency
of
neutral
expression
in
young children who screened positive on the M-CHAT
. Together, these preliminary results
support the use of engaging brief movies shown on a
cost-effective tablet, combined with automated CVA
behavioral coding, as an objective and feasible tool for
measuring an early emerging symptom of ASD, namely,
increased frequency of neutral facial expressions.
While the predictive power of emotional expression in
some of the videos varied, all but one represent a medium
effect, equivalent to Cohen’s d = 0.5 or greater [Rice &
Harris, 2005]. Overall, the best predictor from our battery
of videos is the “Rhymes” video, which had an AUC with
a large effect size (equivalent to d > 0.8). While this may
suggest that presenting the “Rhymes” video alone is sufﬁ-
cient for differentiating between the ASD and non-ASD
groups, we caution readers from coming to this conclu-
sion for two reasons: First, it is possible that, had we had
a larger sample, the other videos would have had a larger
effect. Second, we anticipate that there will be variability
in the ASD group with regard to which features a single
child will express and different videos may be better
suited to elicit different features in any given individual.
As such, we believe that it is important to understand
how each independent feature, in this case facial affect,
performs across the different videos so that we can being
to build better predictive models from combinations of
features [e.g., facial affect and postural sway as described
in ].
To further understand the difference of facial expres-
sion in the non-ASD group as compared to our ASD sam-
ple, we explored the facial landmarks differentiating
between the Other facial expression category that domi-
nated the non-ASD control group versus the Neutral
facial expression, which was more common in the ASD
group. Through this analysis, we identiﬁed the features of
raised eyebrows and open mouth to play a role in dis-
criminating between the Other vs. Neutral categories.
This facial pattern is consistent with an engaged/inter-
ested look displayed when a child is actively watching, as
described in young children by Sullivan and Lewis [2003].
It is interesting to note a raised pitch angle was also statis-
tically signiﬁcant. Since the median difference of this
angle between the two facial expression is small (3.2�),
this
may
be
a
natural
movement
of
raising
one’s
eyebrows.
Our results need to be considered in light of several
limitations. First, the CVA models of facial expressions
used in the current study were trained on adult faces
. Despite this, our previous ﬁndings
with young children demonstrate good concordance
between human and CVA coding on the designation of
facial expressions . Furthermore,
the Other facial expression category includes all non-
positive or negative expressions. As such, even though
we were able to determine the predominant feature driv-
ing those expressions was the raised eyebrows, which is
in line with our observations from watching the movies,

Comparison of ASD Versus Non-ASD: Area Under the
Curve (AUC) Analyses
AUC without covariates
AUC with age in the model
Bubbles 1
Bunny
Puppets
Rhymes
Bubbles 2
INSAR
Carpenter et al./Digital behavioral phenotyping in ASD

it is possible that there are a combination of facial expres-
sions in the non-ASD group driving this designation.
Future studies will need to train on the engaged/inter-
ested facial expression speciﬁcally and test the robustness
of this ﬁnding. Additionally, though we have previously
demonstrated good reliability between our CVA algo-
rithms
and
human
coding
of
emotions
[Hashemi
et al., 2015], future validation of our CVA analysis of
emotional facial expressions in larger datasets is currently
underway. Second, using the LASSO statistical approach
means our model may not select all features that have dif-
ferentiating
information.
However,
we
selected
this
approach because it minimizes over ﬁtting the model.
Third, our sample size was relatively small and we do not
have separate training and testing samples. To account
for this, we applied cross-validation on the ROC curves
even though this decreased the performance metrics of
the model. This suggests that our ROC results are poten-
tially conservative. Finally, our comparison group con-
tains
both
children
with
typical
development
and
children with non-ASD developmental delays, a factor
that can be viewed as both a weakness and a strength.
Previous research has demonstrated that increased fre-
quency of neutral expressions does differentiate children
with ASD from those with other developmental delays
[Bieberich & Morgan, 2004; ]. How-
ever, due to the small sample size of children with non-
ASD developmental delays, we were unable to directly
test this in our data. Furthermore, because only a subset
of the sample received an assessment of cognitive ability,
it is possible that there were additional children in the
non-ASD comparison group that also had a developmen-
tal delay that was undetected. Ongoing research in a pro-
spective,
longitudinal
study
with
larger
samples
is
underway to further parse the ability of our CVA tools to
differentiate between children with ASD, children with a
non-ASD developmental delay and/or attention deﬁcit
hyperactivity disorder, and typically developing children.
While a difference in facial expression is one core fea-
ture of ASD, the heterogeneity in ASD means we do not
expect all children with ASD to display this sign of ASD.
As such, our next step is to combine the current results
with other measures of autism risk assessed through the
current digital screening tool, including response to
name , postural sway [Dawson
et al., 2018], and differential vocalizations [Tenenbaum
et al., 2020], among other features, to develop a risk score
based on multiple behaviors [Dawson & Sapiro, 2019].
Since no one child is expected to display every risk behav-
ior, a goal is to determine thresholds based on the total
number of behaviors, regardless of which combination of
behaviors, to asses for risk. This is similar to what is done
in commonly used screening and diagnostic tools, such
as the M-CHAT , Autism Diagnostic

Analysis of Other Facial Expression. The 4 panels on the left depict heat maps of aligned landmarks across ASD and non-ASD
participants when they were exhibiting Neutral and other facial expressions (the color bar indicates proportion of frames where land-
marks were displayed in a given image location). The single panel on the right is an example of the landmark distances explored.
INSAR
Carpenter et al./Digital behavioral phenotyping in ASD

Interview [Lord, Rutter, & Le Couteur, 1994], and ADOS
[Gotham, Risi, Pickles, & Lord, 2007; ].
In summary, we evaluated an integrated, objective tool
for the elicitation and measurement of facial movements
and expressions in toddlers with and without ASD. The
current study adds to a body of research supporting digi-
tal behavioral phenotyping as a viable method for
assessing autism risk behaviors. Our goal is to further
develop and validate this tool so that it can eventually be
used within the context of current standard of care to
enhance autism screening in pediatric populations.
Acknowledgments
Funding for this work was provided by NIH R01-MH
(Sapiro,
Dawson
PIs),
NIH
(Dawson, Sapiro PIs), NICHD P50HD093074 (Dawson,
Kollins, PIs), Simons Foundation (Sapiro, Dawson, PIs),
Duke Department of Psychiatry and Behavioral Sciences
PRIDe award (Dawson, PI), Duke Education and Human
Development Initiative, Duke-Coulter Translational Part-
nership Grant Program, National Science Foundation, a
Stylli Translational Neuroscience Award, and the Depart-
ment of Defense. Some of the stimuli used for the movies
were created by Geraldine Dawson, Michael Murias, and
Sara Webb at the University of Washington. This work
would not have been possible without the help of Eliza-
beth Glenn, Elizabeth Adler, and Samuel Marsan. We also
gratefully acknowledge the participation of the children
and families in this study. Finally, we could not have
completed this study without the assistance and collabo-
ration of Duke pediatric primary care providers.
Conﬂict of interests
Guillermo Sapiro has received basic research gifts from
Amazon, Google, Cisco, and Microsoft and is a consul-
tant for Apple and Volvo. Geraldine Dawson is on the Sci-
entiﬁc
Advisory
Boards
of
Janssen
Research
and
Development, Akili, Inc., LabCorp, Inc., Tris Pharma, and
Roche Pharmaceutical Company, a consultant for Apple,
Inc, Gerson Lehrman Group, Guidepoint, Inc., Teva Phar-
maceuticals, and Axial Ventures, has received grant
funding from Janssen Research and Development, and is
CEO of DASIO, LLC (with Guillermo Sapiro). Dawson
receives royalties from Guilford Press, Springer, and
Oxford University Press. Dawson, Sapiro, Carpenter,
Hashemi, Campbell, Espinosa, Baker, and Egger helped
develop aspects of the technology that is being used in
the study. The technology has been licensed and Daw-
son, Sapiro, Carpenter, Hashemi, Espinosa, Baker, Egger,
and Duke University have beneﬁted ﬁnancially.